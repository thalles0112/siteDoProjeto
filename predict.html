<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
    <link rel="shortcut icon" href="img/logo-3.png" type="image/x-icon">
    <title>Projeto Integrador</title>
</head>
<body>

    <div class='title'> 
        <a href="index.html"> <h1>Projeto Integrador</h1> </a>
        <img style="height: 100px" src="img/logo-3.png">
    </div>
    


    
    
    <div class="topico">
        <h2>Estrutura Básica</h2>
        <ul>
            <a href="dataset.html">   <li>Estrutura do Banco de dados</li>          </a>
            <a href="preprocess.html"><li>Preprocessamento de imagens (frames)</li> </a>
            <a href="RN.html">        <li>Treinamento da rede neural</li>           </a>
            <a href="predict.html">   <li>Leitura da camera</li>                    </a>
        </ul>
    </div>


    <div class="topico">
        <h2>Abrir a câmera e usar a rede neural treinada para ler a camera e traduzir os sinais em texto</h2>
        <div class='code'>
        <pre><code>
        import cv2 as cv
        from tensorflow.keras.models import load_model
        import numpy as np
        
        labels = ['oi', 'boa', 'tarde']
        
        MODE = 'mod' # normal, mod
        
        
        vid_name = ''
        
        model = load_model('modelconv2d-3') # unnoficial
        #cap = cv.VideoCapture(opa+vid_name)
        cap = cv.VideoCapture(0)
        
        while True:
            ret, frame = cap.read()
            frame_ = cv.resize(frame, (300,300))
            gray = cv.cvtColor(frame_, cv.COLOR_BGR2GRAY)
            gray= cv.GaussianBlur(gray, (3, 3),0)
            gray = cv.Canny(gray, 110, 135)
            sobelx = cv.Sobel(gray, cv.CV_64F, 1,0)
            sobely = cv.Sobel(gray, cv.CV_64F, 0,1)
            combined_ = cv.bitwise_or(sobelx, sobely)
            
            combined_p = np.array(combined_, 'float32')
            combined_p = np.expand_dims(combined_p, 0)
            combined_p= np.expand_dims(combined_p, -1)
        
            pred = np.argmax(model.predict(combined_p))
            print(pred)
            if MODE == 'mod':
                cv.putText(combined_, labels[pred], (30,50), cv.FONT_HERSHEY_COMPLEX, 3, (255,0,0), 3)
                cv.imshow('smile!', combined_)
            elif MODE == 'normal':
                cv.putText(frame, labels[pred], (30,50), cv.FONT_HERSHEY_COMPLEX, 3, (255,0,0), 3)
                cv.imshow('smile!', frame)
        
            
            
            cv.waitKey(1)
        
        cap.release()
        cv.destroyAllWindows()
        
            </code></pre>
        </div>
    </div>

    <div class="topico-texto">
        <p>
            Aqui usaremos a camera e a rede neural para que o programa "adivinhe" em tempo real qual sinal de libras o usuário está fazendo em frente o dispositivo e retornar o resultado em forma de texto.
        </p>
        <p>
            Como a parte da interface gráfica ainda não está pronta, o resultado da
            "adivinhação" é posta na janela do openCV apenas para testes e validação da rede neural. Posteriormente o openCV será usado apenas para ler o vídeo da câmera e processar os efeitos.
        </p>
    </div>

    <footer>
        <div>
            veja o codigo clicando <a target='new' href="https://github.com/thalles0112/libras">aqui</a>
        </div>    
    </footer>

</body>
</html>