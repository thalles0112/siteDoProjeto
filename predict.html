<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
    <link rel="shortcut icon" href="img/logo-3.png" type="image/x-icon">
    <title>Projeto Integrador</title>
</head>
<body>

    <div class='title'> 
        <a href="index.html"> <h1>Projeto Integrador</h1> </a>
        <img style="height: 100px" src="img/logo-3.png">
    </div>
    


    
    
    <div class="topico">
        <h2>Estrutura Básica</h2>
        <ul>
            <a href="dataset.html">   <li>Estrutura do Banco de dados</li>          </a>
            <a href="preprocess.html"><li>Preprocessamento de imagens (frames)</li> </a>
            <a href="RN.html">        <li>Treinamento da rede neural</li>           </a>
            <a href="predict.html">   <li>Leitura da camera</li>                    </a>
        </ul>
    </div>


    <div class="topico">
        <h2>Abrir a camera e usar a rede neural treinada para ler a camera e traduzir os sinais em texto</h2>
        <img src="img/predict.png" alt="">
    </div>

    <div class="topico-texto">
        <p>
            Aqui usaremos a camera e a rede neural em tempo real para que o programa "adivinhe" em tempo real qual sinal de libras o usuário está fazendo em frente a camera e retornar o resultado para a interface gráfica.
        </p>
        <p>
            Como a parte da interface gráfica ainda não está pronta, o resultado da
            prediction é posta na janela do openCV apenas para testes. Posteriormente o openCV será usado apenas para ler o vídeo da câmera e processar os efeitos. O video que será mostrado ao usuário será da classe do kivy.
        </p>
    </div>

    <footer>
        <div>
            veja o codigo clicando <a target='new' href="https://github.com/thalles0112/libras">aqui</a>
        </div>
    </footer>

</body>
</html>